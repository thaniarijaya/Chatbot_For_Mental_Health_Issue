# -*- coding: utf-8 -*-
"""kiwil 2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vjBOfkDO8-FbnJi_91vLrEQ-oDjiPzyv
"""

!pip install keras

#IMPORT LIBRARIES
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout 
from tensorflow.keras.optimizers import SGD
import random

import nltk #natural language processing lib
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
import json
import pickle

intents_file = open('intents.json').read()
intents = json.loads(intents_file)

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

words=[]
classes = []
documents = []
ignore_letters = ['!', '?', ',', '.']
for intent in intents['intents']:
    for pattern in intent['patterns']:
        #tokenize each word
        word = nltk.word_tokenize(pattern)
        words.extend(word)        
        #add documents in the corpus
        documents.append((word, intent['tag']))
        # add to our classes list
        if intent['tag'] not in classes:
          classes.append(intent['tag'])
print(words)

print(classes)

#menghilangkan ignore words
for i in documents:
  for j in i[0]:
    if j in ignore_letters:
      i[0].remove(j)
print(documents)

for i in words:
  if i in ignore_letters:
    words.remove(i)
words

#LEMMATIZATION --> ambil kata dasar
# lemmatize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words] #words = isinya kata-kata yang sudah dipecah
words = sorted(list(set(words)))

# sort classes (intent)
classes = sorted(list(set(classes)))

# documents = combination between patterns and tags
print (len(documents), "documents")

# classes = intents
print (len(classes), "classes", classes)

# words = all words, vocabulary
print (len(words), "unique lemmatized words", words)

#membuka file pkl
pickle.dump(words,open('words.pkl','wb'))
pickle.dump(classes,open('classes.pkl','wb'))

# create the training data
training = []
# membuat array sepanjang jumlah 'CLASS (intent)' yang kita miliki
output_empty = [0] * len(classes)

# MEMBUAT TRAINING SET --> bag of words utk setiap kalimat
for doc in documents: 
    bag = []
    # mengambil setiap pola kalimat yg ada
    word_patterns = doc[0]
    # semua kata di 'lemmatize' --> yaitu diambil kata dasarnya
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]

    # jika kata ditemukan di pattern yang sedang dicek, maka assign angka '1', selain itu assign angka '0'
    for word in words: #ngeloop sejumlah jumlah 'words' yg kita punya
        bag.append(1) if word in word_patterns else bag.append(0)

    #CLASSES --> dimasukkan ke output row
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    training.append([bag, output_row])

# shuffle the features and make numpy array
#random.shuffle(training)
training = np.array(training)
# create training and testing lists. X patterns - , Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])
print("Training data is created")

# NEURAL NETWORK MODEL
model = Sequential() 
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu')) #train x = nilai binary dari pattern kata-kata
model.add(Dropout(0.5)) #mencegah overfitting -> koneksi neuron dikurangi
model.add(Dense(64, activation='relu')) 
model.add(Dropout(0.5)) 
model.add(Dense(len(train_y[0]), activation='softmax')) # Compiling model. SGD with Nesterov accelerated gradient gives good results for this model 
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) 
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) #Training and saving the model 
hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=5, verbose=1) 
model.save('chatbot_model.h5', hist) 
print('model is created')

from keras.models import load_model

def clean_up_sentence(sentence):
    # tokenize kalimat yang masuk
    sentence_words = nltk.word_tokenize(sentence)
    # lemmatization
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

#membuat bag of words berisi value 1 (untuk kata yang ada di bag) & 0 (yg tdk ada)
def bag_of_words(sentence, words, show_details=True):
    # tokenization dgn cara melemparkan ke fungsi clean_up_sentence
    sentence_words = clean_up_sentence(sentence)
    # bag of words - vocabulary matrix
    bag = [0]*len(words)  
    for s in sentence_words:
        #mengambil index (i) & elemen (word) di dalam list words
        for i,word in enumerate(words):
            if word == s: 
                # mengisi index dimana word ditemukan dengan angka '1'
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % word)
    return(np.array(bag))

def predict_class(sentence):
    # filter below threshold predictions
    p = bag_of_words(sentence, words,show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]
    # sorting strength probability
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
    return return_list

def getResponse(ints, intents_json):
    tag = ints[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if(i['tag']== tag):
            result = random.choice(i['responses'])
            break
    return result

print("Kiwil: Selamat datang!")
print("*Untuk mengakhiri obrolan, ketik 'bye kiwil'*")
print("Seperti pepatah tak kenal maka tak sayang, kenalin aku Kiwil. Nama kamu siapa?")
nama = input('Nama Kamu: ')
print("Kiwil: Nama yang cantik. Ayuk, mulai obrolan ini!")
msg = input(nama+': ')
end_trigger = ["bye kiwil", "bai kiwil", "selamat tinggal kiwil", "sampai jumpa kiwil", "babai kiwil", "babai" "bai", "dada"]
while msg not in end_trigger:
  inp = predict_class(msg)
  res = getResponse(inp, intents)
  print('Kiwil: ', res)
  print
  msg = input(nama+": ")
  msg = msg.lower()
print("Kiwil: See you", nama, "!")